---
title: "데이터과학을 위한 통계 6장"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 기초설정
```{r}
library(tidyverse)
```


## 1. KNN 알고리즘

1. 특징들이 가장 유사한(예측변수들이 유사한) K개의 레코드를 찾는다.

2. 이 유사한 레코드들 중에 다수가 속한 클래스가 무엇인지 찾은 후에 새로운 레코드를 그 클래스에 할당한다.

3. 예측(KNN 회귀): 유사한 레코드들의 평균을 찾아서 새로운 레코드에 대한 예측값으로 사용한다.

회귀와는 달리 모델을 피팅하는 과정이 필요없는 간편한 예측/분류 방법이다.

모든 예측변수는 **수치형**이어야 한다. 


```{r}
library(FNN)
loan200 <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\loan200.csv")
loan200 <- as.data.frame(loan200)

# 첫째행을 실험값으로 사용
newloan <- loan200[1, 2:3, drop=FALSE]

# knn 적합
knn_pred <- knn(train = loan200[-1, 2:3], test=newloan, cl = loan200[-1, 1], k=20)

# 실험값의 결과 확인하기
knn_pred == "paid off"
```


knn에서 유사성은 **거리 지표**를 통해 결정된다. 두 벡터 사이에 가장 많이 사용되는 지표는 **유클리드 거리**이다. 두 벡터 사이의 차이에 대한 제곱합을 구한 뒤 그 값의 제곱근을 취한다.

다음으로 많이 사용하는 거리는 **맨하탄 거리**이다. 맨하탄 거리는 점과 점 사이의 이동 시간으로 급접성을 따질 때 좋은 지표가 된다.


#### 표준화

표준화 혹은 정규화란, 모든 변수에서 평균을 빼고 표준편차로 나누는 과정을 통해 변수들을 모두 비슷한 스케일에 넣는 것이다. 이러한 방식은 실제 측정된 값의 스케일 때문에 모델에 심한 영향을 주는 것을 막을 수 있다.

표준화값을 보통 **Z값**이라고 부른다.

KNN이나 다른 알고리즘에서는 데이터를 미리 표준화하는 것이 필수이다.



이 아래 두 코드 이해를 잘 못했음.

```{r}
loan <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\loan_data.csv")

# 새로운 실험값(revol_bal 값의 단위가 큼)
newloan <- loan[1, c("payment_inc_ratio", "dti", "revol_bal", "revol_util"), drop=FALSE]

outcome <- loan$outcome

# -1은 절편을 제외하겠다는 뜻임.
loan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal + revol_util,
                        data=loan)

knn_pred <- knn(train = loan_df, test = newloan, cl=outcome, k=5)

loan_df[attr(knn_pred, "nn.index"), ]
```


```{r}
# scale 함수를 이용한 데이터 표준화
loan_std <- scale(loan_df)
newloan_std <- loan_std[1, c("payment_inc_ratio", "dti", "revol_bal", "revol_util"), drop=FALSE]

knn_pred <- knn(train = loan_std, test = newloan_std, cl=outcome, k=5)

# 표준화 후 새롭게 얻은 5개의 최근접 이웃들은 모든 변수에서 훨씬 더 유사해졌다.
loan_df[attr(knn_pred, "nn.index"), ]
```


#### K 선택하기

k를 잘 선택하는 것은 KNN의 성능을 결정하는 중요한 요소이다. 가장 간단한 방법은 K = 1로 두는 것이다. 이는 1-최근접 이웃 분류기가 된다. 새로 들어온 데이터와 가장 가까운 데이터를 찾아 예측 결과로 사용한다.

일반적으로 K가 너무 작으면 오버피팅 문제가 발생한다. 반대로 K가 너무 크면 결정 함수가 과하게 평탄화되어 KNN의 예측 기능을 잃어버리게 된다.

보통 K를 1에서 20 사이에 놓는다. 동률이 나오는 경우를 막기 위해 보통은 홀수를 사용한다.


#### knn을 통한 피처 엔지니어링

KNN은 실용적인 측면에서, 다른 분류 방법들의 특정 단계에 사용할 수 있게 모델에 **지역적 정보**를 추가할 수 있다.

- KNN 은 데이터에 기반하여 분류 결과(클래스에 속할 확률)를 얻는다.
- 이 결과는 해당 레코드에 새로운 특징(피처)으로 추가된다. 이 결과를 다른 분류 방법에 사용한다.

```{r}
# 대출자의 신용정보를 나타내는 피처 만들기
borrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc + delinq_2yrs_zero + pub_rec_zero, data = loan)

borrow_knn <- knn(borrow_df, test = borrow_df, cl=loan$outcome, prob = TRUE, k=10)

prob <- attr(borrow_knn, "prob")

borrow_feature <- ifelse(borrow_knn=="default", prob, 1-prob)

summary(borrow_feature)
```


신용 기록을 기초로 대출자가 대출을 갚지 못할 것으로 예상되는 정도를 나타내는 피처를 만들었다.

참고로 **model.matrix**는 선형 모형에 적합하기 좋은 행렬로 변환시켜주는 함수이다.

formula를 넣으면 그거에 맞는 데이터프레임을 반환시켜주는 함수같다.











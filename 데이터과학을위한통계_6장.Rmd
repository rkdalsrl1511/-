---
title: "데이터과학을 위한 통계 6장"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 기초설정
```{r}
library(tidyverse)
```


## 1. KNN 알고리즘

1. 특징들이 가장 유사한(예측변수들이 유사한) K개의 레코드를 찾는다.

2. 이 유사한 레코드들 중에 다수가 속한 클래스가 무엇인지 찾은 후에 새로운 레코드를 그 클래스에 할당한다.

3. 예측(KNN 회귀): 유사한 레코드들의 평균을 찾아서 새로운 레코드에 대한 예측값으로 사용한다.

회귀와는 달리 모델을 피팅하는 과정이 필요없는 간편한 예측/분류 방법이다.

모든 예측변수는 **수치형**이어야 한다. 


```{r}
library(FNN)
loan200 <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\loan200.csv")
loan200 <- as.data.frame(loan200)

# 첫째행을 실험값으로 사용
newloan <- loan200[1, 2:3, drop=FALSE]

# knn 적합
knn_pred <- knn(train = loan200[-1, 2:3], test=newloan, cl = loan200[-1, 1], k=20)

# 실험값의 결과 확인하기
knn_pred == "paid off"
```


knn에서 유사성은 **거리 지표**를 통해 결정된다. 두 벡터 사이에 가장 많이 사용되는 지표는 **유클리드 거리**이다. 두 벡터 사이의 차이에 대한 제곱합을 구한 뒤 그 값의 제곱근을 취한다.

다음으로 많이 사용하는 거리는 **맨하탄 거리**이다. 맨하탄 거리는 점과 점 사이의 이동 시간으로 급접성을 따질 때 좋은 지표가 된다.


#### 표준화

표준화 혹은 정규화란, 모든 변수에서 평균을 빼고 표준편차로 나누는 과정을 통해 변수들을 모두 비슷한 스케일에 넣는 것이다. 이러한 방식은 실제 측정된 값의 스케일 때문에 모델에 심한 영향을 주는 것을 막을 수 있다.

표준화값을 보통 **Z값**이라고 부른다.

KNN이나 다른 알고리즘에서는 데이터를 미리 표준화하는 것이 필수이다.



이 아래 두 코드 이해를 잘 못했음.

```{r}
loan <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\loan_data.csv")

# 새로운 실험값(revol_bal 값의 단위가 큼)
newloan <- loan[1, c("payment_inc_ratio", "dti", "revol_bal", "revol_util"), drop=FALSE]

outcome <- loan$outcome

# -1은 절편을 제외하겠다는 뜻임.
loan_df <- model.matrix(~ -1 + payment_inc_ratio + dti + revol_bal + revol_util,
                        data=loan)

knn_pred <- knn(train = loan_df, test = newloan, cl=outcome, k=5)

loan_df[attr(knn_pred, "nn.index"), ]
```


```{r}
# scale 함수를 이용한 데이터 표준화
loan_std <- scale(loan_df)
newloan_std <- loan_std[1, c("payment_inc_ratio", "dti", "revol_bal", "revol_util"), drop=FALSE]

knn_pred <- knn(train = loan_std, test = newloan_std, cl=outcome, k=5)

# 표준화 후 새롭게 얻은 5개의 최근접 이웃들은 모든 변수에서 훨씬 더 유사해졌다.
loan_df[attr(knn_pred, "nn.index"), ]
```


#### K 선택하기

k를 잘 선택하는 것은 KNN의 성능을 결정하는 중요한 요소이다. 가장 간단한 방법은 K = 1로 두는 것이다. 이는 1-최근접 이웃 분류기가 된다. 새로 들어온 데이터와 가장 가까운 데이터를 찾아 예측 결과로 사용한다.

일반적으로 K가 너무 작으면 오버피팅 문제가 발생한다. 반대로 K가 너무 크면 결정 함수가 과하게 평탄화되어 KNN의 예측 기능을 잃어버리게 된다.

보통 K를 1에서 20 사이에 놓는다. 동률이 나오는 경우를 막기 위해 보통은 홀수를 사용한다.


#### knn을 통한 피처 엔지니어링

KNN은 실용적인 측면에서, 다른 분류 방법들의 특정 단계에 사용할 수 있게 모델에 **지역적 정보**를 추가할 수 있다.

- KNN 은 데이터에 기반하여 분류 결과(클래스에 속할 확률)를 얻는다.
- 이 결과는 해당 레코드에 새로운 특징(피처)으로 추가된다. 이 결과를 다른 분류 방법에 사용한다.

```{r}
# 대출자의 신용정보를 나타내는 피처 만들기
borrow_df <- model.matrix(~ -1 + dti + revol_bal + revol_util + open_acc + delinq_2yrs_zero + pub_rec_zero, data = loan)

borrow_knn <- knn(borrow_df, test = borrow_df, cl=loan$outcome, prob = TRUE, k=10)

prob <- attr(borrow_knn, "prob")

borrow_feature <- ifelse(borrow_knn=="default", prob, 1-prob)

summary(borrow_feature)
```


신용 기록을 기초로 대출자가 대출을 갚지 못할 것으로 예상되는 정도를 나타내는 피처를 만들었다.

참고로 **model.matrix**는 선형 모형에 적합하기 좋은 행렬로 변환시켜주는 함수이다.

formula를 넣으면 그거에 맞는 데이터프레임을 반환시켜주는 함수같다.


## 2. 트리 모델

회귀 및 분류 트리(CART), 의사 결정 트리라고 부르며, 랜덤 포레스트 및 부스팅 방식과 같은 강력한 방법들이 파생됐다.

```{r}
# 간단한 트리모형 만들고, 그래프 보기
library(rpart)

loan_tree <- rpart(outcome ~ borrower_score + payment_inc_ratio,
                   data = loan, control = rpart.control(cp=.005))

plot(loan_tree, uniform = TRUE, margin = .05)
text(loan_tree)

summary(loan_tree)
```

트리의 각 분할 영역에 대한 동질성(클래스 순도), 불순도를 측정할 수 있다.

**지니 불순도**와 **엔트로피**가 대표적인 불순도 측정 지표이다.

트리의 형성이 길어질수록 오버피팅의 위험이 있기 때문에 가지치기를 한다. 

복잡도 파라미터 cp를 이용하면, 어떤 크기의 트리가 새로운 데이터에 대해 가장 좋은 성능을 보일지 추정할 수 있다. cp가 작다면, 오버피팅, cp가 크다면 예측 능력을 갖질 못한다.

트리의 장점은 **블랙박스 모형**이 아니라는 점이다.

따라서 다음과 같은 장점을 갖는다.

- 데이터 탐색을 위한 시각화가 가능하다. 데이터간 비선형 관계도 담아낼 수 있기 때문에, 변수 간에 어떤 관계가 있고, 어떤 변수가 중요한지를 확인할 수 있다.
- 일종의 규칙들의 집합이라고 볼 수 있기 때문에, 비전문가들과 대화하는데 효과적이다.


## 3. 배깅과 랜덤 포레스트

단일 모델을 엄청나게 많이 만들어놓고, 다수결 투표 혹은 다중 모델의 평균을 내는 방식으로 최종 모델을 결정하는 것이 **앙상블 모델**이다.

**배깅**과 **부스팅**은 앙상블 모델을 만드는 방법이다. 

배깅은 다양한 모델들을 정확히 같은 데이터에 대해 구하는 대신, 매번 부트스트랩 재표본에 대해 새로운 모델을 만든다.

랜덤 포레스트는 배깅 방법을 적용한 모델이다. 

```{r}
library(randomForest)

loan3000 <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\loan3000.csv")

loan3000 <- as.data.frame(loan3000)

# 문자형을 수치로 변환
loan3000$outcome <- ifelse(loan3000$outcome == "paid off",
                           1, 0)

# 범주형 자료로 변환
loan3000$outcome <- as.factor(loan3000$outcome)

# 랜덤 포레스트 모델
rf <- randomForest(outcome ~ borrower_score + payment_inc_ratio,
                   data = loan3000)

# 예측
pred <- predict(rf)
summary(pred)
```

랜덤포레스트는 블랙박스 모델이다. 직관적인 해석은 불가능하다. 또한 오버피팅의 위험성이 다소 높다고 볼 수 있다.

```{r}
# 랜덤포레스트를 통한 변수 중요도 찾기
rf_all <- randomForest(outcome ~.,
                       data = loan3000,
                       importance = TRUE)

rf_all$importance

varImpPlot(rf_all, type=1)
varImpPlot(rf_all, type=2)
```

해당 변수가 예측에 미치는 영향력이 제거 되었을 때, 감소하는 정확도와 지니 불순도 점수의 감소량을 측정한다.

#### 하이퍼 파라미터

다른 딥러닝/머신러닝 알고리즘들과 마찬가지로, 랜덤 포레스트는 성능을 조절할 수 있는 손잡이가 달려있다.

이러한 손잡이를 **하이퍼파라미터**라고 부른다. 

- nodesize : 말단 노드의 크기를 의미한다. 분류를 위한 기본 설정은 1이며, 회귀 문제에서는 5이다.
- maxnodes : 각 결정 트리에서 전체 노드의 최대 개수를 의미한다. 기본적인 제한이 없고, 다만 nodesize 제한 설정에 따라 가장 큰 트리의 크기가 결정된다.

nodesize와 maxnodes를 크게 하면 더 작은 트리를 얻게되고, 오버피팅을 피할 수 있게 된다.








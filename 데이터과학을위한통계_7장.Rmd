---
title: "데이터과학을 위한 통계 7장"
output: rmarkdown::github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# 기초설정
```{r}
library(tidyverse)
```

비지도 학습은, 레이블이 달린 데이터를 이용해 모델을 학습하는 과정없이 데이터로부터 의미를 이끌어내는 통계적 기법들을 의미한다.

- 클러스터링 : 데이터의 의미 있는 그룹들을 찾기
- 차원 줄이기 : 데이터 변수들을 관리할 수 있을 만한 수준으로 줄이기

등의 다양한 용도로 사용될 수 있다.

## 1. 주성분분석(PCA)

수치형 변수가 어떤 식으로 공변하는지 알아내는 기법.

```{r}
sp500 <- read_csv(file = "C:\\Users\\rkdal\\OneDrive\\바탕 화면\\월간R프로젝트\\데이터과학을위한통계\\psds_data\\sp500_data.csv")

oil_px <- sp500[, c("CVX", "XOM")]
pca <- princomp(oil_px)

pca$loadings

ggplot(data = oil_px, aes(x=CVX, y=XOM)) +
  geom_point(alpha=.3) +
  stat_ellipse(type = "norm", level = .99)
```

변수를 3개 이상으로 확장하는 것도 간단하다. 첫 성분의 선형 결합 수식에 그냥 예측변수를 추가하기만 하면 된다.

#### 주성분 해석

주성분의 상대적 중요도를 표시해주는 스크리그래프를 통해 시각화할 수 있다.

```{r}
syms <- c("AAPL", "MSFT", "CSCO", "INTC", "CVX", "XOM", "SLB", "JPM", "WFC", "USB", "AXP", "WMT", "TGT", "HD", "COST")

top_sp <- sp500[sp500$X1>="2005-01-01", syms]

sp_pca <- princomp(top_sp)

screeplot(sp_pca)

sp_pca$scores
```

첫 번째 주성분의 변동이 가장 크고 나머지 상위 주성분일수록 중요한 것을 볼 수 있다.


## 2. K 평균 클러스터링

클러스터링(군집화)은 데이터를 서로 다른 그룹으로 분류하는 기술을 말한다.

그룹들은 예측변수 혹은 결과변수로 사용할 수 있다.

K 평균은 데이터를 K개의 클러스터로 나눈다.
할당된 클러스터의 평균과 포함된 데이터들의 거리 제곱합이 최소가 되도록 나눈다.

이 방법을 수행할 때에는 데이터를 정규화해두는 것이 좋다. 정규화를 하지 않으면 스케일이 가장 큰 변수가 클러스터링 과정을 독점한다.

**정규화** : 데이터 값에서 평균을 빼고 표준편차로 나누어주는 방법

```{r}
df <- sp500[sp500$X1 >="2011-01-01", c("XOM", "CVX")]

# K 평균 클러스터링 수행
km <- kmeans(df, centers = 4)

# 클러스터열 추가
df$cluster <- factor(km$cluster)

head(df)

# 클러스터 평균 확인
centers <- data.frame(cluster=factor(1:4), km$centers)

centers

# 클러스터 평균을 포함하여 시각화하기
ggplot(data=df, aes(x=XOM, y=CVX, color=cluster, shape=cluster)) +
  geom_point(alpha=.3) +
  geom_point(data=centers, aes(x=XOM, y=CVX), size=3, stroke=2)
```

k 평균 알고리즘은 사용자가 미리 정해준 K값과 클러스터 평균의 초깃값을 가지고 알고리즘을 시작한다.

- 각 레코드를 거리가 가장 가까운 평균을 갖는 클러스터에 할당한다.
- 새로 할당된 레코드들을 가지고 새로운 클러스터 평균을 계산한다.

해당 알고리즘에서는 보통 각 레코드의 K개의 클러스터들 가운데 하나에 랜덤하게 할당한 후 그렇게 얻은 클러스터들의 평균을 사용한다.

랜덤하게 초깃값을 변화시켜가며 해를 구할 수도 있다. nstar 변수를 이용해 랜덤하게 초깃값을 다르게 설정해 알고리즘을 시행할 횟수를 설정할 수 있다.

```{r}
# 초깃값을 다르게 설정하여 10회 수행
syms <- c("AAPL", "MSFT", "CSCO", "INTC", "CVX", "XOM", "SLB", "JPM", "WFC", "USB", "AXP", "WMT", "TGT", "HD", "COST")

df <- sp500[sp500$X1 >="2011-01-01", syms]

km <- kmeans(df, centers = 5, nstart=10)

# 각 클러스터들의 크기
km$size
```

#### 클러스터 개수 선정

적용되는 문제에 따라서 적절한 개수를 지정해줘야 한다. 통계적 접근 방식도 사용할 수 있다.

팔꿈치 방법은 언제 클러스터 세트가 데이터의 분산의 "대부분"을 설명하는지를 알려준다. 누적 분산이 가파르게 상승한 다음 어느 순간 평평하게 되는 지점을 말하며 이러한 성질 때문에 팔꿈치라는 이름이 붙었다.

```{r}
pct_var <- data.frame(pct_var = 0,
                      num_clusters = 2:14)

totalss <- kmeans(df,
                  centers = 14,
                  nstart = 50,
                  iter.max = 100)$totss

for (i in 2:14) {
  
  pct_var[i-1, "pct_var"] <- kmeans(df, centers=i, nstart = 50, iter.max = 100)$betweenss/totalss
  
}

pct_var
```

해당 예제에서는 분산이 증가율이 너무 서서히 떨어지기 때문에 눈에 띄는 위치가 없는 경우이다.

일반적으로 클러스터의 개수를 정확히 얻는 방법은 없다. 교차타당성검사 등을 동원하여 결정하는 것이 바람직하다.







